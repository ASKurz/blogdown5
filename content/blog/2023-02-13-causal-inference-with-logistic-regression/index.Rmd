---
title: Causal inference with logistic regression
subtitle: 'Part 3 of the GLM and causal inference series.'
author: A. Solomon Kurz
date: '2023-02-13'
excerpt: "In this third post of the causal inference series, we switch to a binary outcome variable. As we will see, some of the nice qualities from the OLS paradigm fall apart when we want to make causal inferences with binomial models."
tags:
  - ANCOVA
  - ANOVA
  - ATE
  - binary
  - binonial
  - CATE
  - causal inference
  - g-computation
  - GLM
  - logistic regression
  - marginal standardization
  - noncollapsibility
  - potential outcomes
  - R
  - RCT
  - tidyverse
  - tutorial
draft: false
layout: single
featured: no
bibliography: /Users/solomonkurz/Dropbox/blogdown5/content/blog/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown5/content/blog/apa.csl  
link-citations: yes
---

```{r, echo = F, cache = F}
# knitr::opts_chunk$set(fig.retina = 2.5)
options(width = 110)
```

So far in this series, we've been been using ordinary least squares (OLS) to analyze and make causal inferences from our experimental data. Though OLS is an applied statistics workhorse and performs admirably in some cases, there are many contexts in which it's just not appropriate. In medical trials, for example, many of the outcome variables are binary. Some typical examples are whether a patient still has the disease (coded `1`) or not (coded `0`), or whether a participant has died (coded `1`) or is still alive (coded `0`). In these cases, we want to model our data with a likelihood function that can handle binary data, and the go-to solution is the binomial[^1]. As we will see, some of the nice qualities from the OLS paradigm fall apart when we want to make causal inferences with binomial models. But no fear; we have solutions.

## We need data

In this post, we'll be borrowing data from @wilson2017internet, *Internet-accessed sexually transmitted infection (e-STI) testing and results service: A randomised, single-blind, controlled trial*. Wilson and colleagues were open-science champions and made their primary data available as supporting information in their `S1Data.xls` file, which you can download from [here](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002479#sec020).

```{r, warning = F, message = F}
# packages
library(tidyverse)
library(marginaleffects)
library(flextable)
library(broom)

# data
wilson2017 <- readxl::read_excel("data/S1Data.xls", sheet = "data")

# what?
glimpse(wilson2017)
```

These data were from a randomized controlled trial in London (2014--2015), which was designed to assess the effectiveness of an internet-accessed sexually transmitted infection testing (e-STI testing) and results service on STI testing uptake and STI cases diagnosed in chlamydia, gonorrhoea, HIV, and syphilis. The 2,072 participants were fluent in English, each had at least 1 sexual partner in the past year, consented to take an STI test, and had access to the internet. From the abstract, we further learn:

> Participants were randomly allocated to receive 1 text message with the web link of an e-STI testing and results service (intervention group) or to receive 1 text message with the web link of a bespoke website listing the locations, contact details, and websites of 7 local sexual health clinics (control group). Participants were free to use any other services or interventions during the study period. The primary outcomes were self-reported STI testing at 6 weeks, verified by patient record checks, and self-reported STI diagnosis at 6 weeks, verified by patient record checks. (p. 1)

In the opening of the Results section (p. 8), we learn 9 more participants were excluded, leaving a total of 2,063 cases in the primary data set, which matches with the row number in the `S1Data.xls` data file. Here's the count, by experimental condition.

```{r}
wilson2017 %>% 
  count(group)
```

For our analyses, we will take `anytest` as the focal variable. This variable indicates whether a participants' medical record indicated any STI testing during at 6 weeks. Here's the breakdown, by experimental group.

```{r}
wilson2017 %>% 
  count(group, anytest) %>% 
  group_by(group) %>% 
  mutate(percent_by_group = round(100 * n / sum(n), digits = 1))
```

As is often the case with real-world data, we have missing values. That problem could be fun to focus on [see @bartlett2023gformla], but that's a task for another day. Walking out causal inference methods for a logistic regression paradigm will be a sufficient challenge, for now.

### Subset.

The methods we'll be exploring in this post will work perfectly fine with the full data set. But it'll actually be easier for me to make some of my points if we reduce the sample size. Here we'll take a random subset of $n = 400$ of the cases with no missing data on the primary outcome variable `anytest`, and a few covariates of interest.

```{r}
set.seed(1)

wilson2017 <- wilson2017 %>% 
  mutate(msm = ifelse(msm == 99, NA, msm)) %>% 
  drop_na(anytest, gender, partners, msm, ethnicgrp, age) %>% 
  slice_sample(n = 400)

# what are the dimensions?
dim(wilson2017)
```

Now we'll adjust some of the variables, themselves. We will save the nominal covariates `gender`, `msm`, and `ethnicgrp` as factors with defined levels. The covariate `partners` is ordinal[^2], but for our purposes it will be fine to convert it to a factor, too. The `age` covariate is continuous, but it'll come in handy to rescale it into a $z$-score metric, which we'll name `agez`. We'll simplify the character variable for out experimental groups, `group`, in to a a `tx` dummy coded `0` for the control condition and `1` for those in the intervention condition. Then we'll rename the `anon_id` index to `id`, reorder the columns, and drop the other columns we won't be focusing on in this post.

```{r}
wilson2017 <- wilson2017 %>% 
  # factors
  mutate(gender    = factor(gender, levels = c("Female", "Male")),
         msm       = factor(msm, levels = c("other", "msm")),
         partners  = factor(partners, levels = c(1:9, "10+")),
         ethnicgrp = factor(ethnicgrp,
                            levels = c("White/ White British", "Asian/ Asian British", "Black/ Black British", "Mixed/ Multiple ethnicity", "Other"))) %>% 
  # z-score
  mutate(agez = (age - mean(age)) / sd(age)) %>% 
  # make a simple treatment dummy
  mutate(tx = ifelse(group == "SH:24", 1, 0)) %>% 
  rename(id = anon_id) %>% 
  select(id, tx, anytest, gender, partners, msm, ethnicgrp, age, agez)

# what do we have?
glimpse(wilson2017)
```

```{r, eval = F, echo = F}
save(wilson2017, file = "data/wilson2017.rda")
```

### Descriptive statistics

We've already introduced our binary outcome variable `anytest` and the experimental treatment dummy `tx`. In the Method section, we further learned the randomization algorithm balanced

> for gender (male, female, transgender), age (16–19, 20–24, 25–30 years), number of sexual partners in last 12 months (1, 2+), and sexual orientation (MSM, all other groups). All factors had equal weight in determining marginal imbalance. (p. 4)
 
Further down in the Method (p. 7), we learn all these variables were used as covariates in the primary analysis[^3], in addition to ethnicity.

To get a sense of these covariates, here we use **flextable** [@R-flextable; @gohelUsingFlextable2023] to make a Table 1 type table of the categorical variables for our randomized subset.

```{r}
wilson2017 %>% 
  pivot_longer(cols = c(gender, partners, msm, ethnicgrp),
               names_to = "variable", values_to = "category") %>% 
  group_by(variable) %>% 
  count(category) %>% 
  mutate(`%` = round(100 * n / sum(n), digits = 1)) %>% 
  # these last 4 lines make the flextable-based table
  as_grouped_data(groups = c("variable")) %>% 
  flextable() %>% 
  autofit() %>% 
  italic(j = 3, part = "header")
```

Though we'll be using the standardized version of `age` in the model, here are the basic descriptive statistics for `age`.

```{r}
wilson2017 %>% 
  summarise(mean = mean(age),
            sd = sd(age),
            min = min(age),
            max = max(age))
```

## Models

In this blog post, we'll be fitting two models to these data. The first will be the unconditional ANOVA-type model

$$
\begin{align*}
\text{anytest}_i & \sim \operatorname{Binomial}(n = 1, p_i) \\
\operatorname{logit}(p_i) & = \beta_0 + \beta_1 \text{tx}_i,
\end{align*}
$$

where $\operatorname{logit}(.)$ indicates we're using the conventional logit link, which is where we get the term "logistic regression." Then we'll fit an ANCOVA-type version including all the covariates:

$$
\begin{align*}
\text{anytest}_i & \sim \operatorname{Binomial}(n = 1, p_i) \\
\operatorname{logit}(p_i) & = \beta_0 + \beta_1 \text{tx}_i \\
& \;\; + \beta_2 \text{agez}_i \\
& \;\; + \beta_3 \text{Male}_i \\
& \;\; + \beta_4 \text{MSM}_i \\
& \;\; + \beta_5 \text{Asian}_i + \beta_6 \text{Black}_i + \beta_7 \text{Mixed}_i + \beta_8 \text{Other}_i \\
& \;\; + \beta_9 \text{partners2}_i + \beta_{10} \text{partners3}_i + \dots + \beta_{17} \text{partners10}\texttt{+}_i,
\end{align*}
$$

where, due to the scoring of the covariates, the reference category would be a person in the control condition, who was of average age (22.9 years), a female not identifying as a man who slept with men, White, and who had been with one sexual partner over the past year. Here's how to fit the models with the base **R** `glm()` function.

```{r}
# ANOVA-type model
glm1 <- glm(
  data = wilson2017,
  family = binomial,
  anytest ~ tx
)

# ANCOVA-type model
glm2 <- glm(
  data = wilson2017,
  family = binomial,
  anytest ~ tx + agez + gender + msm + ethnicgrp + partners
)

# summarize
summary(glm1)
summary(glm2)
```

## ATE for the ANOVA

### $\beta_1$ in the logistic regression ANOVA.

As a first step, let's extract the $\beta_1$ estimate, with its standard error and so on, with `broom::tidy()`.

```{r}
tidy(glm1, conf.int = T) %>% 
  filter(term == "tx")
```

This $\beta_1$ estimate is on the log-odds scale, which isn't the most intuitive and can take some time to master. Though this won't work for the standard error, test statistic and $p$-value, you can exponentiate the point estimate and 95% confidence intervals to convert them to an odds-ratio metric.

```{r}
tidy(glm1, conf.int = T) %>% 
  filter(term == "tx") %>% 
  select(estimate, starts_with("conf.")) %>% 
  mutate_all(exp)
```

Odds ratios range from 0 to positive infinity, and have an inflection point at 1. Though I don't care for them, odds ratios seem to be popular effect sizes among medical researchers[^4]. To each their own. But if you're like me, you want to convert the results of the model to the metric of a difference in probability[^5]. A naïve data analyst might try to convert $\beta_1$ out of the log-odds metric into the probability metric with the base **R** `plogis()`.

```{r}
tidy(glm1, conf.int = T) %>% 
  filter(term == "tx") %>% 
  select(estimate, starts_with("conf.")) %>% 
  mutate_all(plogis)
```

This, however, this approach DOES NOT convert $\beta_1$ into a difference in probability. This is not an average treatment effect, and sadly, it's completely uninterpretable. As Imbens and Ruben put it: "The average treatment effect cannot be expressed directly in terms of the parameters of the logistic or probit regression model" [-@imbensCausalInferenceStatistics2015, p. 128]. But we can use an *in*direct method to compute the point estimate for the ATE with a combination of both $\beta_0$ and $\beta_1$, and the `plogis()` function.

```{r}
plogis(coef(glm1)[1] + coef(glm1)[2]) - plogis(coef(glm1)[1])
```

In somewhat awkward statistical notation, that code is

$$\operatorname{logit}^{-1}(\beta_0 + \beta_1) - \operatorname{logit}^{-1}(\beta_1),$$

where $\operatorname{logit}^{-1}(\cdot)$ is the inverse of the logistic function. In base **R**, `plogis()` is the $\operatorname{logit}^{-1}(\cdot)$ function. Let's prove our estimate with this formula is correct with the *sample* ATE (SATE), as computed by hand with sample statistics.

```{r}
wilson2017 %>% 
  group_by(tx) %>% 
  summarise(p = mean(anytest == 1)) %>% 
  pivot_wider(names_from = tx, values_from = p) %>% 
  mutate(ate = `1` - `0`)
```

Unlike with OLS-type models, you cannot compute the ATE in a logistic-regression context with $\beta_1$ alone. You need to account for the other parameters in the model, too.

### Compute $\Pr (y_i^1 = 1) - \Pr(y_i^0 = 1)$ from `glm1`.

Back in the [last post](http://localhost:4321/blog/2023-02-06-causal-inference-with-potential-outcomes-bootcamp/), we leaned we could compute the average treatment effect, $\tau_\text{ATE}$, in two ways:

$$\tau_\text{ATE} = \mathbb E (y_i^1 - y_i^0) = \mathbb E (y_i^1) - \mathbb E (y_i^0),$$

where, for the moment, we're excluding covariates from the framework. As it turns out, these equalities hold regardless of whether $y_i$ is of a continuous or binary variable. If we focus on the second method, $\mathbb E (y_i^1)$ and $\mathbb E (y_i^0)$ are probabilities for binary variables. To walk that out in statistical notation,

$$
\begin{align*}
\mathbb E (y_i^1) & = \Pr (y_i^1 = 1),\ \text{and} \\
\mathbb E (y_i^0) & = \Pr (y_i^0 = 1),
\end{align*}
$$

which means that

$$\mathbb E (y_i^1) - \mathbb E (y_i^0) = \Pr (y_i^1 = 1) - \Pr(y_i^0 = 1).$$

To take the notation even further, we typically use $p$ in place of $\Pr()$ when working with the binomial likelihood. Therefore

$$
\begin{align*}
\mathbb E (y_i^1) & = \Pr (y_i^1 = 1) = p^1,\ \text{and} \\
\mathbb E (y_i^0) & = \Pr (y_i^0 = 1) = p^0,
\end{align*}
$$

and finally

$$
\begin{align*}
\mathbb E (y_i^1) - \mathbb E (y_i^0) & = \Pr (y_i^1 = 1) - \Pr(y_i^0 = 1) \\
                                      & = p^1 - p^0.
\end{align*}
$$

This is all important because within the context of our binomial regression model, we can compute the population estimates for $p^1$, $p^0$, and their difference. Thus in the case of the binomial ANOVA model,

$$\tau_\text{ATE} = p^1 - p^0.$$

If you just wanted to compute the contrast between the two group-level probabilities, the base **R** `predict()` approach might be a good place to start. Here we define a simple data frame with the two levels of the `tx` dummy, and pump the values into `predict()`.

```{r}
nd <- tibble(tx = 0:1)

# log odds metric
predict(glm1, 
        newdata = nd,
        se.fit = TRUE) %>% 
  data.frame() %>% 
  bind_cols(nd)
```

Note how the default behavior is to return the estimates and their standard errors in the log-odds metric. Also note that when working with binomial models, `predict()` will not return 95% confidence intervals. If you want the estimate in the probability metric, you can set `type = "response"`.

```{r}
# probability metric
predict(glm1, 
        newdata = nd,
        se.fit = TRUE,
        type = "response") %>% 
  data.frame() %>% 
  bind_cols(nd)
```

And just to check, here's how those estimates match up with the sample statistics.

```{r}
wilson2017 %>% 
  group_by(tx) %>% 
  summarise(p = mean(anytest == 1))
```

However, this approach gives us no way to compute the contrast of those probabilities in a way that retains the uncertainty information in the standard errors. For that, we turn once again to the **marginaleffects** package. To start, we can use the `predictions()` function to return the group probabilities, along with their measures of uncertainty.

```{r}
predictions(glm1, newdata = nd, by = "tx")
```

Notice that `predictions()` returns probabilities by default, rather than log odds. To get the contrast for the two probabilities, just add `hypothesis = "revpairwise"`.

```{r}
predictions(glm1, newdata = nd, by = "tx", hypothesis = "revpairwise")
```

Not only do we get the probability contrast, but we get the standard error and confidence intervals, too.

### Compute $\mathbb E (p_i^1 - p_i^0)$ from `glm1`.

Within the context of our ANOVA-type binomial model, the $\mathbb E (y_i^1 - y_i^0)$ method still works fine for estimating the ATE. But there are new conceptual quirks with which we must contend. First, unlike with continuous variables, there are only four possible combinations of $y_i^1$ and $y_i^0$, and there are only three possible values for $\tau_i$. 

```{r}
crossing(y0 = 0:1, 
         y1 = 0:1) %>% 
  mutate(tau = y1 - y0) %>% 
  flextable()
```

Imbens and Rubin discussed this kind of scenario in Section 1.3 in their [-@imbensCausalInferenceStatistics2015] text. If we were in a context where we could compute the raw $y_i^1 - y_i^0$ contrasts with synthetic data, the average of those values,

$$\tau_\text{SATE} = \frac{1}{N} \sum_{i=1}^N (y_i^1 - y_i^0),$$

could take on any continuous value ranging from -1 to 1. Thus unlike with the OLS paradigm for continuous variables, the metric for $\tau_\text{SATE}$, and also $\tau_\text{ATE}$, is not the same as the metric for and individual case's causal effect $\tau_i$. The average of a set of integers is a real number.

The second issue is when we compute the case-specific counterfactual estimates from a logistic regression model, we don't typically get a vector of $\hat y_i^1$ and $\hat y_i^1$ values; we get $\hat p_i^1$ and $\hat p_i^0$ instead. Let's explore with `predict()`.

```{r}
# redefine the data grid
nd <- wilson2017 %>% 
  select(id) %>% 
  expand_grid(tx = 0:1)

# compute
predict(glm1, 
        newdata = nd,
        se.fit = TRUE,
        # request the probability metric
        type = "response") %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  # look at the first 6 rows
  head()
```

The `fit` column contains the $\hat p_i$ values, rather than $\hat y_i$ values. However, it turns out that when you take the average of the contrast of these values, you still get an estimate of the ATE. Thus within the context of our logistic regression model,

$$\tau_\text{ATE} = \mathbb E (y_i^1 - y_i^0) = {\color{blueviolet}{\mathbb E (p_i^1 - p_i^0)}}.$$

Here's how to compute the point estimate for $\tau_\text{ATE}$ via $\mathbb E (p_i^1 - p_i^0)$ with `predict()`.

```{r}
predict(glm1, 
        newdata = nd,
        se.fit = TRUE,
        type = "response") %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  select(id, tx, fit) %>% 
  pivot_wider(names_from = tx, values_from = fit) %>% 
  summarise(ate = mean(`1` - `0`))
```

We can compute a standard error for that estimate with the `avg_comparisons()` function from the **marginaleffects** package [see @arelBundock2023CausalInference].

```{r}
avg_comparisons(glm1, variables = list(tx = 0:1))
```

If this seems like a weird bait-and-switch, and you wanted more evidence that $\mathbb E (y_i^1 - y_i^0) = \mathbb E (p_i^1 - p_i^0)$, we could always simulate. Let's go back to our `predict()` workflow. After we've computed the various $\hat p_i$ values, we can use the `rbinom()` function to probabilistically simulate a vector of $\hat y_i$ values. Then we just need to wrangle and summarize the results.
  
```{r}
set.seed(1)

nd %>% 
  mutate(p = predict(glm1, newdata = nd, type = "response")) %>% 
  # simulate y
  mutate(y = rbinom(n = n(), size = 1, prob = p)) %>% 
  select(-p) %>% 
  pivot_wider(names_from = tx, values_from = y) %>% 
  summarise(ate = mean(`1` - `0`))
```

At first glance, this might look like a failure. 0.158 is a much lower value than 0.19 from above. But keep in mind that this was a summary of a single iteration of a random process. In the next code block, we'll expand the initial data set so that each participant has 1,000 iterations of both $\hat y_i^1$ and $\hat y_i^0$ values. We'll compute $\mathbb E (y_i^1 - y_i^0)$ within each iteration, and them summarize the results from the 1,000 iterations by their mean and standard deviation.

```{r}
set.seed(1)

nd %>% 
  mutate(p = predict(glm1, newdata = nd, type = "response")) %>% 
  # make 1,000 iterations
  expand_grid(iteration = 1:1000) %>% 
  mutate(y = rbinom(n = n(), size = 1, prob = p)) %>% 
  select(-p) %>% 
  pivot_wider(names_from = tx, values_from = y) %>% 
  group_by(iteration) %>% 
  # summarize within iterations
  summarise(ate_iteration = mean(`1` - `0`)) %>% 
  # summarize across the iterations
  summarise(mean = mean(ate_iteration),
            sd = sd(ate_iteration))
```

The mean[^6] of our random process is a pretty good approximation of $\tau_\text{ATE}$ computed from the `avg_comparisons()` function, above.

So in the case of an ANOVA-type logistic regression model of a randomized experiment,

* $\operatorname{logit}^{-1}(\beta_0 + \beta_1) - \operatorname{logit}^{-1}(\beta_1)$,
* $p^1 - p^0$, and
* $\mathbb E (p_i^1 - p_i^0)$

are all the same thing. They're all estimators of our estimand $\tau_\text{ATE}$, the average treatment effect.

## ATE for the ANCOVA

In our [last post](http://localhost:4321/blog/2023-02-06-causal-inference-with-potential-outcomes-bootcamp/), we focused on a case where the only covariate was continuous. In this blog post, we're analyzing a data set containing a mixture of continuous and discrete covariates. So for notation sake, let $\mathbf C_i$ stand a vector of *continuous* covariates and let $\mathbf D_i$ stand a vector of *discrete* covariates, both of which vary across the $i$ cases. We can use these to help estimate the ATE with the formula:

$$\tau_\text{ATE} = \mathbb E (y_i^1 - y_i^0 | \mathbf C_i, \mathbf D_i).$$

In words, this means the average treatment effect in the population is the same as the average of each person's individual treatment effect, computed conditional on their continuous covariates $\mathbf C_i$ and discrete covariates $\mathbf D_i$. This, again, is sometimes called *standardization* or *g-computation*. Within the context of a logistic regression model, we further observe

$$
\tau_\text{ATE} = \mathbb E (y_i^1 - y_i^0 | \mathbf C_i, \mathbf D_i) = {\color{blueviolet}{\mathbb E (p_i^1 - p_i^0 | \mathbf C_i, \mathbf D_i)}},
$$

where $p_i^1$ and $p_i^0$ are the counterfactual probabilities for each of the $i$ cases, estimated in light of their covariate values. 

Whether we have continuous covariates, discrete covariates, or a combination of both, the standardization method works the same. However, this is no longer the case when using the difference in population means approach, the covariate-adjusted version of $\mathbb E (y_i^1) - \mathbb E (y_i^0)$. One complication is we might not be able to mean-center the discrete covariates in our $\mathbf D$ vector. Sometimes people will mean center dummy variables, which can lead to awkward interpretive issues[^7]. But even this approach will not generalize well to multi-categorical nominal variables, like ethnicity. Another solution is to set discrete covariates at their modes [see @muller2014estimating], which we'll denote $\mathbf D^m$. This gives us a new estimand:

$$\tau_\text{TEMM} = \operatorname{\mathbb{E}} \left (y_i^1 | \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (y_i^0 | \mathbf{\bar C}, \mathbf D^m \right),$$

where *TEMM* is an acronym for *treatment effect at the mean and/or mode*. Beware the TEMM acronym is not widely used in the literature; I'm just using it here to help clarify a point. More importantly, once you move beyond the ATE to specify particular values for $\mathbf C$ and/or $\mathbf D$, you're really just computing one form or another of the *conditional average treatment effect* (CATE; $\tau_\text{CATE}$), which we might clarify with the formula

$$\tau_\text{CATE} = \operatorname{\mathbb{E}}(y_i^1 | \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}}(y_i^0 | \mathbf C = \mathbf c, \mathbf D = \mathbf d),$$

where $\mathbf C = \mathbf c$ is meant to convey you have chosen particular values $\mathbf c$ for the variables in the $\mathbf C$ vector, and $\mathbf D = \mathbf d$ is meant to convey you have chosen particular values $\mathbf d$ for the variables in the $\mathbf D$ vector. In addition to means or modes, these values could be any which are of particular interest to researchers and their audiences.

Within the context of a logistic regression model, we further observe

$$
\begin{align*}
\tau_\text{TEMM} & = \operatorname{\mathbb{E}} \left (y_i^1 | \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (y_i^0 | \mathbf{\bar C}, \mathbf D^m \right) \\
& = {\color{blueviolet}{\operatorname{\mathbb{E}} \left (p_i^1 | \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (p_i^0 | \mathbf{\bar C}, \mathbf D^m \right)}}
\end{align*}
$$

where $p_i^1$ and $p_i^0$ are the counterfactual probabilities for each of the $i$ cases, estimated in light of their covariate values. In a similar way

$$
\begin{align*}
\tau_\text{CATE} & = \operatorname{\mathbb{E}} (y_i^1 | \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}}(y_i^0 | \mathbf C = \mathbf c, \mathbf D = \mathbf d) \\
& = {\color{blueviolet}{\operatorname{\mathbb{E}} (p_i^1 | \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}}(p_i^0 | \mathbf C = \mathbf c, \mathbf D = \mathbf d)}}.
\end{align*}
$$

Importantly, we have some inequalities to consider:

$$
\begin{align*}
\mathbb E (y_i^1 - y_i^0 | \mathbf C_i, \mathbf D_i) & \neq \operatorname{\mathbb{E}} \left (y_i^1 | \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (y_i^0 | \mathbf{\bar C}, \mathbf D^m \right) \\
& \neq \operatorname{\mathbb{E}} (y_i^1 | \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}} (y_i^0 | \mathbf C = \mathbf c, \mathbf D = \mathbf d)
\end{align*}
$$

and thus

$$
\begin{align*}
\mathbb E (p_i^1 - p_i^0 | \mathbf C_i, \mathbf D_i) & \neq \operatorname{\mathbb{E}} \left (p_i^1 | \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (p_i^0 | \mathbf{\bar C}, \mathbf D^m \right) \\
& \neq \operatorname{\mathbb{E}} (p_i^1 | \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}} (p_i^0 | \mathbf C = \mathbf c, \mathbf D = \mathbf d),
\end{align*}
$$

which means

$$
\begin{align*}
\tau_\text{ATE} & \neq \tau_\text{TEMM} \\
& \neq \tau_\text{CATE}.
\end{align*}
$$

As we will see, this holds for logistic regression models regardless of whether you have discrete covariates.

### $\beta_1$ in the logistic regression ANCOVA.

Earlier we learned the coefficient for the experimental group, $\beta_1$, does not have a direct relation with the ATE for the logistic regression ANOVA model. In a similar way, the $\beta_1$ coefficient does not have a direct relation with the ATE for the logistic regression ANCOVA model, either. If you want the ATE, you'll have to use the methods from the sections to come. In the meantime, let's compare the $\beta_1$ estimates for the ANOVA and ANCOVA models:

```{r}
 bind_rows(tidy(glm1), tidy(glm2)) %>% 
  filter(term == "tx") %>% 
  mutate(fit = c("glm1", "glm2"),
         model_type = c("ANOVA", "ANCOVA")) %>%
  rename(`beta[1]` = estimate) %>% 
  select(fit, model_type, `beta[1]`, std.error)
```

Unlike what typically occurs with OLS-based models, the standard error for $\beta_1$ *increased* when we added the baseline covariates to the model. It turns out this will generally happen with logistic regression models, even when using high-quality covariates [see @robinson1991someSurprising]. This does not, however, mean we should not use baseline covariates in our logistic regression models. Rather, it means that we need to focus on how to compute the ATE, rather than fixate on the model coefficients [cf. @daniel2021makingApples]. This can be very unsettling for those with strong roots in the OLS framework--it was for me. All I can say is: Your OLS sensibilities will not help you, here. The sooner you shed them, the better.

### Compute $\operatorname{\mathbb{E}} \left (p_i^1 | \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (p_i^0 | \mathbf{\bar C}, \mathbf D^m \right)$ from `glm2`.

With our ANCOVA-type `glm2` model, we can compute $\operatorname{\mathbb{E}} \left (p_i^1 | \mathbf{\bar C}, \mathbf D^m \right)$ and $\operatorname{\mathbb{E}} \left (p_i^1 | \mathbf{\bar C}, \mathbf D^m \right)$ with the base **R** `predict()` function. As a first step, we'll define our prediction grid with the sample mean for our continuous covariate `agez`, the sample modes for our four discrete covariates, and then expand the grid to include both values of the `experimental` dummy. This presents a small difficulty, however, because base **R** does not have a function for modes. Here we'll make one ourselves.

```{r}
get_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

This `get_mode()` function is used internally by the **marginaleffects** package (see [here](https://github.com/vincentarelbundock/marginaleffects/blob/9a06aa03c017947df978caa4d82fa6e650e2de8f/R/mean_or_mode.R#L4)), and has its origins in [this](https://stackoverflow.com/a/8189441/342331) stackoverflow discussion. Here's how we can use `get_mode()` to help us make the `nd` data grid.

```{r}
nd <- wilson2017 %>% 
  summarise(agez      = 0,  # recall agez is a z-score, with a mean of 0 by definition
            gender    = get_mode(gender),
            msm       = get_mode(msm),
            ethnicgrp = get_mode(ethnicgrp),
            partners  = get_mode(partners)) %>% 
  expand_grid(tx = 0:1)

# what is this?
print(nd)
```

Thus we will be computing our estimate for $\tau_\text{TEMM}$ based on a White 23-year-old woman who had one partner over the past year. By definition, such a person would not be a man who has sex with men (`msm == 1`). Also, we know this person is 23 years old because `agez == 0` at that value. Here's the proof.

```{r}
wilson2017 %>% 
  summarise(mean_age = mean(age))
```

Now we pump these values into `predict()`.

```{r}
predict(glm2, 
        newdata = nd,
        se.fit = TRUE,
        type = "response") %>% 
  data.frame() %>% 
  bind_cols(nd)
```

To get the contrast with standard errors and so on, we switch to the `predictions()` function and set `hypothesis = "revpairwise"`.

```{r}
# conditional probabilities
predictions(glm2, newdata = nd, by = "tx")

# TEMM
predictions(glm2, newdata = nd, by = "tx", hypothesis = "revpairwise")
```

Thus we expect our hypothetical person with demographics at the mean and/or modes for the covariates will be about 22% more likely to get tested if given the intervention, compared to if she had not.

### Compute $\operatorname{\mathbb{E}} (p_i^1 | \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}} (p_i^0 | \mathbf C = \mathbf c, \mathbf D = \mathbf d)$ from `glm2`.

Since the $\tau_\text{TEMM}$ is just a special case of a $\tau_\text{CATE}$, we might practice computing our estimate for $\tau_\text{CATE}$ with a different set of covariate values. Since men who have sex with men (MSM) were one of the vulnerable subgroups of interest in @wilson2017internet, we might take a look to see which combination of covariate values was most common for MSM in our subset of the data.

```{r}
wilson2017 %>% 
  filter(msm == "msm") %>% 
  count(age, agez, ethnicgrp, partners) %>% 
  arrange(desc(n))
```

It appears we're now interested in computing $\tau_\text{CATE}$ for a White 26-year-old MSM who had 10 or more partners over the past year. Let's redefine our `nd` predictor grid.
 
```{r}
nd <- wilson2017 %>% 
  filter(msm == "msm") %>% 
  count(age, agez, gender, msm, ethnicgrp, partners) %>% 
  arrange(desc(n)) %>% 
  slice(1) %>% 
  select(-n) %>% 
  expand_grid(tx = 0:1)

# what now?
print(nd)
```

Now use `predictions()` to compute the counterfactual probabilities and the $\tau_\text{CATE}$.

```{r}
# conditional probabilities
predictions(glm2, newdata = nd, by = "tx")

# CATE
predictions(glm2, newdata = nd, by = "tx", hypothesis = "revpairwise")
```

Turns out this $\tau_\text{CATE}$ is a little larger than our estimate for $\tau_\text{TEMM}$, from above. With this framework, you can compute $\tau_\text{CATE}$  estimates for any number of theoretically-meaningful covariate sets.

### Compute $\mathbb E (p_i^1 - p_i^0 | \mathbf C_i, \mathbf D_i)$ from `glm2`.

Before we compute our counterfactual $\mathbb{E}(p_i^1 - p_i^0 | \mathbf C_i, \mathbf D_i)$ estimates from our ANCOVA-type logistic regression model `glm2`, we'll first need to redefine our `nd` predictor data. This time, we'll retain the full set of covariate values for each participant.

```{r}
nd <- wilson2017 %>% 
  select(id, age, agez, gender, msm, ethnicgrp, partners) %>% 
  expand_grid(tx = 0:1)

# what?
glimpse(nd)
```

Instead of first practicing computing the probabilities with base **R** `predict()`, let's just jump directly to the `precitions()` and `comparisons()` functions from the **marginaleffects** package.

```{r}
# here are the probabilities
predictions(glm2, newdata = nd) %>% 
  head(n = 10)

# here are the contrasts based on those probabilities
comparisons(glm2, newdata = nd, variables = "tx") %>% 
  head(n = 10)
```

Even among the first 10 rows, we can see there's a lot of diversity among the estimates for the individual treatment effects. Before we compute the ATE, it might be worth the effort to look at all those estimates in a coefficient plot.

```{r, fig.width = 8, fig.height = 5}
comparisons(glm2, newdata = nd, variables = "tx") %>% 
  # convert the output to a data frame
  data.frame() %>% 
  # sort the output by the point estimates
  arrange(estimate) %>% 
  # make an index for the ranks
  mutate(rank = 1:n()) %>% 
  
  # plot!
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = rank)) +
  geom_pointrange(linewidth = 1/10, fatten = 1/10) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Behold the diversity among the individual treatment effect estimates.",
       x = expression(hat(tau)[italic(i)]~("i.e., "*hat(italic(p))[italic(i)]^1-hat(italic(p))[italic(i)]^0))) +
  coord_cartesian(xlim = c(-0.1, 0.4)) +
  theme_gray(base_size = 13) +
  theme(panel.grid = element_blank())
```

Going by point estimate, the participant-specific treatment effects range from zero to about .26. The bulk of the estimates are in the upper part of the range, between .2 and .25. Further, the shape of this distribution will depend on the distribution of the levels of the covariates in the sample.

Now here's $\tau_\text{ATE}$ for this model, and for the simpler ANOVA-type `glm1`.

```{r}
bind_rows(
  avg_comparisons(glm1, newdata = nd, variables = "tx"),
  avg_comparisons(glm2, newdata = nd, variables = "tx")
) %>% 
  data.frame() %>% 
  mutate(fit = c("glm1", "glm2"),
         model_type = c("ANOVA", "ANCOVA")) %>%
  rename(`tau[ATE]` = estimate) %>% 
  select(fit, model_type, `tau[ATE]`, std.error)
```

Whereas the standard error for the $\beta_1$ coefficient *increased* when we added the baseline covariates to the model, the standard error for our primary estimand $\tau_\text{ATE}$ *decreased*. This isn't a fluke of our $n = 400$ subset. The same general pattern holds for the full data set. Not only is $\beta_1$ not the same as the ATE for a logistic regression model, adding covariates can have the reverse effect on their respective standard errors. This phenomena is related to the so-called noncollapsibility issue, which is well known among statisticians who work with medical trials. For an entry point into that literature, see @daniel2021makingApples or @morris2022planning. But anyway, yes, baseline covariates can help increase the precision with which you estimate the ATE from a logistic regression model. Don't worry about what happens with $\beta_1$. Focus on the ATE.

## Acknowledgments

I became aware of @wilson2017internet through the follow-up paper by @morris2022planning. Morris and colleagues compared several ways to analyze these data, one of which was the standardization approach for logistic regression, such as we have done here. However, Morris and colleagues used a STATA-based workflow for their paper, and it was [A. Jordan Nafa](https://www.ajordannafa.com/)'s kind efforts (see [here](https://github.com/ajnafa/morris-et-al-2022-replication)) which helped me understand how to use these methods in **R**.

## Recap

In this post, some of the main points we covered were:

* With logistic regression, the $\beta_1$ coefficient has no direct relationship with the ATE, regardless of whether you have included covariates.
* For the logistic regression ANOVA model,
  - $\tau_\text{ATE} = \mathbb E (p_i^1 - p_i^0)$, and
  - $\tau_\text{ATE} = p^1 - p^0$.
* For the logistic regression ANCOVA model,
  - $\tau_\text{ATE} = \mathbb E (p_i^1 - p_i^0 | \mathbf C_i, \mathbf D_i)$, but
  - $\tau_\text{CATE} = \operatorname{\mathbb{E}} (p_i^1 | \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}}(p_i^0 | \mathbf C = \mathbf c, \mathbf D = \mathbf d)$.
* For a logistic regression ANCOVA model, there can be many different values for the conditional average treatment effect, $\tau_\text{CATE}$, depending which values one uses for the covariates. 
* With logistic regression models, baseline covariates tend to
  - *in*crease the standard errors for the $\beta_1$ coefficient, and
  - *de*crease the standard errors for the average treatment effect, $\tau_\text{ATE}$.

In the [next post](https://timely-flan-2986f4.netlify.app/blog/2023-02-15-causal-inference-with-bayesian-models/), we'll explore how our causal inference methods work within an applied Bayesian statistics framework. We'll practice with both simple Gaussian models, and logistic regression models, too. Until then, happy modeling, friends!

## Session information

```{r}
sessionInfo()
```

## References

[^1]: Yes, you geeks, I know we could also use the Bernoulli distribution. But the binomial is much more popular and if we're going to rely on the nice base **R** `glm()` function, we'll be setting `family = binomial`. There is no option for `family = bernoulli`.

[^2]: I suppose you could even argue it's a censored count. But since we'll be using it as a predictor, I'm not sure that argument would be of much help.

[^3]: As it turns out, statisticians and quanty researchers are not in total agreement on whether or how one must condition on covariates when those covariates were used to balance during the randomization process. For a lively twitter discussion on this very data set, see the replies to [this twitter poll](https://twitter.com/SolomonKurz/status/1623349977786228736).

[^4]: There are numerous effect sizes one could compute from a logistic regression model. For a more exhaustive list, as applied within our causal inference framework, see Section 3.3 in @brumback2022Fundamentals.

[^5]: In some parts of the literature, probabilities are called "risks" and differences in probabilities are called "risk differences" [e.g., @morris2022planning]. We will not be using the jargon of "risk" in this blog series.

[^6]: Note that the standard deviation, here, isn't quite the same thing as a standard error. We'd need to do something more akin to bootstrapping, for that. However, this kind of a workflow does have some things in common with the Monte-Carlo-based Bayesian methods we'll be practicing later in this series.

[^7]: For example, say you have a dummy variable called `male`, which is a zero for women and a one for men. One way to interpret a model using the centered version of `male` is it returns the contrast weighted by the proportion of women/men in the sample or population. Another interpretation is this returns the contrast for someone who is in the middle of the female-male spectrum--which is what? Intersex? Non-binary? Transgender? It might be possible to interpret such a computation with skill and care, but such an approach might also leave one's audience confused or offended.

```{r, fig.width = 4, fig.height = 4, echo = F, eval = F}
comparisons(glm2, newdata = nd, variables = "tx") %>% 
  data.frame() %>% 
  
  ggplot(aes(x = estimate)) +
  tidybayes::geom_dots(layout = "swarm", 
                       fill = "magenta4", color = "magenta4") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "The individual treatment effect distribution",
       x = expression(hat(tau)[italic(i)]~("i.e., "*hat(italic(p))[italic(i)]^1-hat(italic(p))[italic(i)]^0))) +
  theme_gray(base_size = 11.7, base_family = "serif") +
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill = "#ffe5ea"))

ggsave("tau-i-featured.jpg", width = 3.5, height = 3.5, units = "in")
```
