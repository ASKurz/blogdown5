---
title: Causal inference with logistic regression
subtitle: 'Part 3 of the GLM and causal inference series.'
author: A. Solomon Kurz
date: '2023-02-13'
excerpt: "In this third post of the causal inference series, we switch to a binary outcome variable. As we will see, some of the nice qualities from the OLS paradigm fall apart when we want to make causal inferences with binomial models."
tags:
  - ANCOVA
  - ANOVA
  - ATE
  - binary
  - binonial
  - CATE
  - causal inference
  - g-computation
  - GLM
  - logistic regression
  - marginal standardization
  - noncollapsibility
  - potential outcomes
  - R
  - RCT
  - tidyverse
  - tutorial
draft: false
layout: single
featured: no
bibliography: /Users/solomonkurz/Dropbox/blogdown5/content/blog/my_blog.bib
biblio-style: apalike
csl: /Users/solomonkurz/Dropbox/blogdown5/content/blog/apa.csl  
link-citations: yes
---

```{r, echo = F, cache = F}
# knitr::opts_chunk$set(fig.retina = 2.5)
options(width = 110)
```

So far in this series, we've been been using ordinary least squares (OLS) to analyze and make causal inferences from our experimental data. Though OLS is an applied statistics workhorse and performs admirably in some cases, there are many contexts in which it's just not appropriate. In medical trials, for example, many of the outcome variables are binary. Some typical examples are whether a patient still has the disease (coded `1`) or not (coded `0`), or whether a participant has died (coded `1`) or is still alive (coded `0`). In these cases, we want to model our data with a likelihood function that can handle binary data, and the go-to solution is the binomial[^1]. As we will see, some of the nice qualities from the OLS paradigm fall apart when we want to make causal inferences with binomial models. But no fear; we have solutions.

## We need data

In this post, we'll be borrowing data from @wilson2017internet, *Internet-accessed sexually transmitted infection (e-STI) testing and results service: A randomised, single-blind, controlled trial*. Wilson and colleagues were open-science champions and made their primary data available as supporting information in their `S1Data.xls` file, which you can download from [here](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002479#sec020).

```{r, warning = F, message = F}
# packages
library(tidyverse)
library(marginaleffects)
# library(flextable)
library(broom)
library(ggdist)
library(patchwork)

# adjust the global theme
theme_set(theme_gray(base_size = 12) +
            theme(panel.grid = element_blank()))

# data
wilson2017 <- readxl::read_excel("data/S1Data.xls", sheet = "data")

# what?
glimpse(wilson2017)
```

These data were from a randomized controlled trial in London (2014--2015), which was designed to assess the effectiveness of an internet-accessed sexually transmitted infection testing (e-STI testing) and results service on STI testing uptake and STI cases diagnosed in chlamydia, gonorrhoea, HIV, and syphilis. The 2,072 participants were fluent in English, each had at least 1 sexual partner in the past year, consented to take an STI test, and had access to the internet. From the abstract, we further learn:

> Participants were randomly allocated to receive 1 text message with the web link of an e-STI testing and results service (intervention group) or to receive 1 text message with the web link of a bespoke website listing the locations, contact details, and websites of 7 local sexual health clinics (control group). Participants were free to use any other services or interventions during the study period. The primary outcomes were self-reported STI testing at 6 weeks, verified by patient record checks, and self-reported STI diagnosis at 6 weeks, verified by patient record checks. (p. 1)

In the opening of the Results section (p. 8), we learn 9 more participants were excluded, leaving a total of 2,063 cases in the primary data set, which matches with the row number in the `S1Data.xls` data file. Here's the count, by experimental condition.

```{r}
wilson2017 %>% 
  count(group)
```

For our analyses, we will take `anytest` as the focal variable. This variable indicates whether a participants' medical record indicated any STI testing during at 6 weeks. Here's the breakdown, by experimental group.

```{r}
wilson2017 %>% 
  count(group, anytest) %>% 
  group_by(group) %>% 
  mutate(percent_by_group = round(100 * n / sum(n), digits = 1))
```

As is often the case with real-world data, we have missing values. That problem could be fun to focus on [see @bartlett2023gformla], but that's a task for another day. Walking out causal inference methods for a logistic regression paradigm will be a sufficient challenge, for now.

### Subset.

The methods we'll be exploring in this post will work perfectly fine with the full data set. But it'll actually be easier for me to make some of my points if we reduce the sample size. Here we'll take a random subset of $n = 400$ of the cases with no missing data on the primary outcome variable `anytest`, and a few covariates of interest.

```{r}
set.seed(1)

wilson2017 <- wilson2017 %>% 
  mutate(msm = ifelse(msm == 99, NA, msm)) %>% 
  drop_na(anytest, gender, partners, msm, ethnicgrp, age) %>% 
  slice_sample(n = 400)

# what are the dimensions?
dim(wilson2017)
```

Now we'll adjust some of the variables, themselves. We will save the nominal covariates `gender`, `msm`, and `ethnicgrp` as factors with defined levels. The covariate `partners` is ordinal[^2], but for our purposes it will be fine to convert it to a factor, too. The `age` covariate is continuous, but it'll come in handy to rescale it into a $z$-score metric, which we'll name `agez`. We'll simplify the character variable for the experimental groups, `group`, in to a a `tx` dummy coded `0` for the control condition and `1` for those in the intervention condition. Then we'll rename the `anon_id` index to `id`, reorder the columns, and drop the other columns we won't be focusing on in this post.

```{r}
wilson2017 <- wilson2017 %>% 
  # factors
  mutate(gender    = factor(gender, levels = c("Female", "Male")),
         msm       = factor(msm, levels = c("other", "msm")),
         partners  = factor(partners, levels = c(1:9, "10+")),
         ethnicgrp = factor(ethnicgrp,
                            levels = c("White/ White British", "Asian/ Asian British", "Black/ Black British", "Mixed/ Multiple ethnicity", "Other"))) %>% 
  # z-score
  mutate(agez = (age - mean(age)) / sd(age)) %>% 
  # make a simple treatment dummy
  mutate(tx = ifelse(group == "SH:24", 1, 0)) %>% 
  rename(id = anon_id) %>% 
  select(id, tx, anytest, gender, partners, msm, ethnicgrp, age, agez)

# what do we have?
glimpse(wilson2017)
```

```{r, eval = F, echo = F}
save(wilson2017, file = "data/wilson2017.rda")
```

### Descriptive statistics

We've already introduced our binary outcome variable `anytest` and the experimental treatment dummy `tx`. In the Method section, we further learned the randomization algorithm balanced

> for gender (male, female, transgender), age (16–19, 20–24, 25–30 years), number of sexual partners in last 12 months (1, 2+), and sexual orientation (MSM, all other groups). All factors had equal weight in determining marginal imbalance. (p. 4)
 
Further down in the Method (p. 7), we learn all these variables were used as covariates in the primary analysis[^3], in addition to ethnicity[^4].

To get a sense of these covariates, we'll make a Table 1 type table[^5] of the categorical variables for our randomized subset.

```{r}
wilson2017 %>% 
  pivot_longer(cols = c(gender, partners, msm, ethnicgrp),
               names_to = "variable", values_to = "category") %>% 
  group_by(variable) %>% 
  count(category) %>% 
  mutate(`%` = round(100 * n / sum(n), digits = 1))
# %>%
#   # these last 4 lines make the flextable-based table
#   as_grouped_data(groups = c("variable")) %>%
#   flextable() %>%
#   autofit() %>%
#   italic(j = 3, part = "header")
```

Though we'll be using the standardized version of `age` in the model, here are the basic descriptive statistics for `age`.

```{r}
wilson2017 %>% 
  summarise(mean = mean(age),
            sd = sd(age),
            min = min(age),
            max = max(age))
```

## Models

In this blog post, we'll be fitting two models to these data. The first will be the unconditional ANOVA-type model

$$
\begin{align*}
\text{anytest}_i & \sim \operatorname{Binomial}(n = 1, p_i) \\
\operatorname{logit}(p_i) & = \beta_0 + \beta_1 \text{tx}_i,
\end{align*}
$$

where $\operatorname{logit}(.)$ indicates we're using the conventional logit link, which is where we get the term "logistic regression." Then we'll fit an ANCOVA-type version including all the covariates:

$$
\begin{align*}
\text{anytest}_i & \sim \operatorname{Binomial}(n = 1, p_i) \\
\operatorname{logit}(p_i) & = \beta_0 + \beta_1 \text{tx}_i \\
& \;\; + \beta_2 \text{agez}_i \\
& \;\; + \beta_3 \text{Male}_i \\
& \;\; + \beta_4 \text{MSM}_i \\
& \;\; + \beta_5 \text{Asian}_i + \beta_6 \text{Black}_i + \beta_7 \text{Mixed}_i + \beta_8 \text{Other}_i \\
& \;\; + \beta_9 \text{partners2}_i + \beta_{10} \text{partners3}_i + \dots + \beta_{17} \text{partners10}\texttt{+}_i,
\end{align*}
$$

where, due to the scoring of the covariates, the reference category would be a person in the control condition, who was of average age (22.9 years), a female not identifying as a man who slept with men, White, and who had been with one sexual partner over the past year. Here's how to fit the models with the base **R** `glm()` function.

```{r}
# ANOVA-type model
glm1 <- glm(
  data = wilson2017,
  family = binomial,
  anytest ~ tx
)

# ANCOVA-type model
glm2 <- glm(
  data = wilson2017,
  family = binomial,
  anytest ~ tx + agez + gender + msm + ethnicgrp + partners
)

# summarize
summary(glm1)
summary(glm2)
```

## ATE for the ANOVA

### $\beta_1$ in the logistic regression ANOVA.

As a first step, let's extract the $\beta_1$ estimate, with its standard error and so on, with `broom::tidy()`.

```{r}
tidy(glm1, conf.int = T) %>% 
  filter(term == "tx")
```

This $\beta_1$ estimate is on the log-odds scale, which isn't the most intuitive and can take some time to master. Though this won't work for the standard error, test statistic and $p$-value, you can exponentiate the point estimate and 95% confidence intervals to convert them to an odds-ratio metric.

```{r}
tidy(glm1, conf.int = T) %>% 
  filter(term == "tx") %>% 
  select(estimate, starts_with("conf.")) %>% 
  mutate_all(exp)
```

Odds ratios range from 0 to positive infinity, and have an inflection point at 1. Though I don't care for them, odds ratios seem to be popular effect sizes among medical researchers[^6]. To each their own. But if you're like me, you want to convert the results of the model to the metric of a difference in probability[^7]. A naïve data analyst might try to convert $\beta_1$ out of the log-odds metric into the probability metric with the base **R** `plogis()`.

```{r}
tidy(glm1, conf.int = T) %>% 
  filter(term == "tx") %>% 
  select(estimate, starts_with("conf.")) %>% 
  mutate_all(plogis)
```

This, however, this approach DOES NOT convert $\beta_1$ into a difference in probability. This is not an average treatment effect, and sadly, it's completely uninterpretable. As Imbens and Ruben put it: "The average treatment effect cannot be expressed directly in terms of the parameters of the logistic or probit regression model" [-@imbensCausalInferenceStatistics2015, p. 128]. But we can use an *in*direct method to compute the point estimate for the ATE with a combination of both $\beta_0$ and $\beta_1$, and the `plogis()` function.

```{r}
plogis(coef(glm1)[1] + coef(glm1)[2]) - plogis(coef(glm1)[1])
```

In somewhat awkward statistical notation, that code is

$$\operatorname{logit}^{-1}(\beta_0 + \beta_1) - \operatorname{logit}^{-1}(\beta_1),$$

where $\operatorname{logit}^{-1}(\cdot)$ is the inverse of the logistic function (i.e., `plogis()`). Let's prove our estimate with this formula is correct with the *sample* ATE (SATE), as computed by hand with sample statistics.

```{r}
wilson2017 %>% 
  group_by(tx) %>% 
  summarise(p = mean(anytest == 1)) %>% 
  pivot_wider(names_from = tx, values_from = p) %>% 
  mutate(ate = `1` - `0`)
```

Unlike with OLS-type models, you cannot compute the ATE in a logistic-regression context with $\beta_1$ alone. You need to account for the other parameters in the model, too.

### Compute $\Pr (y_i^1 = 1) - \Pr(y_i^0 = 1)$ from `glm1`.

Back in the [last post](http://localhost:4321/blog/2023-02-06-causal-inference-with-potential-outcomes-bootcamp/), we leaned we could compute the average treatment effect, $\tau_\text{ATE}$, in two ways:

$$\tau_\text{ATE} = \mathbb E (y_i^1 - y_i^0) = \mathbb E (y_i^1) - \mathbb E (y_i^0),$$

where, for the moment, we're excluding covariates from the framework. As it turns out, these equalities hold regardless of whether $y_i$ is of a continuous or binary variable. If we focus on the second method, $\mathbb E (y_i^1)$ and $\mathbb E (y_i^0)$ are probabilities for binary variables. To walk that out in statistical notation,

$$
\begin{align*}
\mathbb E (y_i^1) & = \Pr (y_i^1 = 1),\ \text{and} \\
\mathbb E (y_i^0) & = \Pr (y_i^0 = 1),
\end{align*}
$$

which means that

$$\mathbb E (y_i^1) - \mathbb E (y_i^0) = \Pr (y_i^1 = 1) - \Pr(y_i^0 = 1).$$

To take the notation even further, we typically use $p$ in place of $\Pr()$ when working with the binomial likelihood. Therefore

$$
\begin{align*}
\mathbb E (y_i^1) & = \Pr (y_i^1 = 1) = p^1,\ \text{and} \\
\mathbb E (y_i^0) & = \Pr (y_i^0 = 1) = p^0,
\end{align*}
$$

and finally

$$
\begin{align*}
\mathbb E (y_i^1) - \mathbb E (y_i^0) & = \Pr (y_i^1 = 1) - \Pr(y_i^0 = 1) \\
                                      & = p^1 - p^0.
\end{align*}
$$

This is all important because within the context of our binomial regression model, we can compute the population estimates for $p^1$, $p^0$, and their difference. Thus in the case of the binomial ANOVA model,

$$\tau_\text{ATE} = p^1 - p^0.$$

If you just wanted to compute the contrast between the two group-level probabilities, the base **R** `predict()` approach might be a good place to start. Here we define a simple data frame with the two levels of the `tx` dummy, and pump the values into `predict()`.

```{r}
nd <- tibble(tx = 0:1)

# log odds metric
predict(glm1, 
        newdata = nd,
        se.fit = TRUE) %>% 
  data.frame() %>% 
  bind_cols(nd)
```

Note how the default behavior is to return the estimates and their standard errors in the log-odds metric. Also note that when working with binomial models, `predict()` will not return 95% confidence intervals. If you want the estimate in the probability metric, you can set `type = "response"`.

```{r}
# probability metric
predict(glm1, 
        newdata = nd,
        se.fit = TRUE,
        type = "response") %>% 
  data.frame() %>% 
  bind_cols(nd)
```

And just to check, here's how those estimates match up with the sample statistics.

```{r}
wilson2017 %>% 
  group_by(tx) %>% 
  summarise(p = mean(anytest == 1))
```

However, this approach gives us no way to compute the contrast of those probabilities in a way that retains the uncertainty information in the standard errors. For that, we turn once again to the **marginaleffects** package. To start, we can use the `predictions()` function to return the group probabilities, along with their measures of uncertainty.

```{r}
predictions(glm1, newdata = nd, by = "tx")
```

Notice that `predictions()` returns probabilities by default, rather than log odds. To get the contrast for the two probabilities, just add `hypothesis = "revpairwise"`.

```{r}
predictions(glm1, newdata = nd, by = "tx", hypothesis = "revpairwise")
```

Not only do we get the probability contrast, but we get the standard error and confidence intervals, too.

### Compute $\mathbb E (p_i^1 - p_i^0)$ from `glm1`.

Within the context of our ANOVA-type binomial model, the $\mathbb E (y_i^1 - y_i^0)$ method still works fine for estimating the ATE. But there are new conceptual quirks with which we must contend. First, unlike with continuous variables, there are only four possible combinations of $y_i^1$ and $y_i^0$, and there are only three possible values for $\tau_i$. 

```{r}
crossing(y0 = 0:1, 
         y1 = 0:1) %>% 
  mutate(tau = y1 - y0) 
# %>% 
#   flextable()
```

Imbens and Rubin discussed this kind of scenario in Section 1.3 in their [-@imbensCausalInferenceStatistics2015] text. If we were in a context where we could compute the raw $y_i^1 - y_i^0$ contrasts with synthetic data, the average of those values,

$$\tau_\text{SATE} = \frac{1}{N} \sum_{i=1}^N (y_i^1 - y_i^0),$$

could take on any continuous value ranging from -1 to 1. Thus unlike with the OLS paradigm for continuous variables, the metric for $\tau_\text{SATE}$, and also $\tau_\text{ATE}$, is not the same as the metric for any individual case's causal effect $\tau_i$. The average of a set of integers is a real number.

The second issue is when we compute the case-specific counterfactual estimates from a logistic regression model, we don't typically get a vector of $\hat y_i^1$ and $\hat y_i^1$ values; we get $\hat p_i^1$ and $\hat p_i^0$ instead. Let's explore with `predict()`.

```{r}
# redefine the data grid
nd <- wilson2017 %>% 
  select(id) %>% 
  expand_grid(tx = 0:1)

# compute
predict(glm1, 
        newdata = nd,
        se.fit = TRUE,
        # request the probability metric
        type = "response") %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  # look at the first 6 rows
  head()
```

The `fit` column contains the $\hat p_i$ values, rather than $\hat y_i$ values. However, it turns out that when you take the average of the contrast of these values, you still get an estimate of the ATE. Thus within the context of our logistic regression model,

$$\tau_\text{ATE} = \mathbb E (y_i^1 - y_i^0) = {\color{blueviolet}{\mathbb E (p_i^1 - p_i^0)}}.$$

Here's how to compute the point estimate for $\tau_\text{ATE}$ via $\mathbb E (p_i^1 - p_i^0)$ with `predict()`.

```{r}
predict(glm1, 
        newdata = nd,
        se.fit = TRUE,
        type = "response") %>% 
  data.frame() %>% 
  bind_cols(nd) %>% 
  select(id, tx, fit) %>% 
  pivot_wider(names_from = tx, values_from = fit) %>% 
  summarise(ate = mean(`1` - `0`))
```

We can compute a standard error for that estimate with the `avg_comparisons()` function from the **marginaleffects** package [see @arelBundock2023CausalInference].

```{r}
avg_comparisons(glm1, variables = list(tx = 0:1))
```

If this seems like a weird bait-and-switch, and you wanted more evidence that $\mathbb E (y_i^1 - y_i^0) = \mathbb E (p_i^1 - p_i^0)$, we could always simulate. Let's go back to our `predict()` workflow. After we've computed the various $\hat p_i$ values, we can use the `rbinom()` function to probabilistically simulate a vector of $\hat y_i$ values. Then we just need to wrangle and summarize the results.
  
```{r}
set.seed(1)

nd %>% 
  mutate(p = predict(glm1, newdata = nd, type = "response")) %>% 
  # simulate y
  mutate(y = rbinom(n = n(), size = 1, prob = p)) %>% 
  select(-p) %>% 
  pivot_wider(names_from = tx, values_from = y) %>% 
  summarise(ate = mean(`1` - `0`))
```

At first glance, this might look like a failure. 0.158 is a much lower value than 0.19 from above. But keep in mind that this was a summary of a single iteration of a random process. In the next code block, we'll expand the initial data set so that each participant has 1,000 iterations of both $\hat y_i^1$ and $\hat y_i^0$ values. We'll compute $\mathbb E (y_i^1 - y_i^0)$ within each iteration, visualize the distribution in a histogram, and then summarize the results from the 1,000 iterations by their mean and standard deviation.

```{r, fig.width = 5, fig.height = 3.25}
# simulate
set.seed(1)

sim <- nd %>% 
  mutate(p = predict(glm1, newdata = nd, type = "response")) %>% 
  # make 1,000 iterations
  expand_grid(iteration = 1:1000) %>% 
  mutate(y = rbinom(n = n(), size = 1, prob = p)) %>% 
  select(-p) %>% 
  pivot_wider(names_from = tx, values_from = y) %>% 
  group_by(iteration) %>% 
  # summarize within iterations
  summarise(ate_iteration = mean(`1` - `0`))

# visualize
sim %>% 
  ggplot(aes(x = ate_iteration)) +
  geom_histogram(binwidth = 0.01) +
  ggtitle("Distribution of model-based simulated ATE's")

# summarize across the iterations
sim %>% 
  summarise(mean = mean(ate_iteration),
            sd = sd(ate_iteration))
```

The mean[^8] of our random process is a pretty good approximation of $\tau_\text{ATE}$ computed from the `avg_comparisons()` function, above.

Backing up a bit, we might want to get a better sense of all those $p_i^1$, $p_i^0$, and $(p_i^1 - p_i^0)$ estimates we've been averaging over. Like in the last post, we'll display them in a couple plots. To keep down the clutter, we'll restrict ourselves to a random $n = 50$ subset of the 400 cases in the `wilson2017` data.

```{r, fig.width = 8, fig.height = 5.5}
# make the random subset
set.seed(3)

id_subset <- wilson2017 %>%
  slice_sample(n = 50) %>% 
  pull(id)

# counterfactual probabilities
p1 <- predictions(glm1, newdata = nd) %>% 
  data.frame() %>% 
  filter(id %in% id_subset) %>% 
  mutate(y = ifelse(tx == 0, "hat(italic(p))^0", "hat(italic(p))^1")) %>% 
  
  ggplot(aes(x = estimate, y = reorder(id, estimate), color = y)) +
  geom_interval(aes(xmin = conf.low, xmax = conf.high),
                position = position_dodge(width = -0.2),
                size = 1/5) +
  geom_point(aes(shape = y),
             size = 2) +
  scale_color_viridis_d(NULL, option = "A", begin = .3, end = .6,
                        labels = scales::parse_format()) +
  scale_shape_manual(NULL, values = c(20, 18),
                     labels = scales::parse_format()) +
  scale_x_continuous(limits = 0:1) +
  scale_y_discrete(breaks = NULL) +
  labs(subtitle = "Counterfactual probabilities",
       x = expression(italic(p[i])),
       y = "id (ranked)") +
  theme(legend.background = element_blank(),
        legend.position = c(.9, .85))

# treatment effects
p2 <- comparisons(glm1, newdata = nd, variables = list(tx = 0:1), by = "id") %>% 
  data.frame() %>% 
  filter(id %in% id_subset) %>% 
  
  ggplot(aes(x = estimate, y = reorder(id, estimate))) +
  geom_vline(xintercept = 0, color = "white") +
  geom_interval(aes(xmin = conf.low, xmax = conf.high),
                size = 1/5) +
  geom_point() +
  scale_x_continuous(limits = c(-0.5, 0.5)) +
  scale_y_discrete(breaks = NULL) +
  labs(subtitle = "Contrasts",
       x = expression(hat(italic(p))[italic(i)]^1-hat(italic(p))[italic(i)]^0),
       y = NULL) +
  theme(legend.background = element_blank(),
        legend.position = c(.9, .85))

# combine the two plots
p1 + p2 + plot_annotation(title = "Person-level estimates based on the logistic regression ANOVA model (glm1)")
```


If the left plot, we see the counterfactual probabilities, depicted by their point estimates (dots) and 95% intervals (horizontal lines), and colored by whether they were based on the control condition $(\hat p_i^0)$ or the experimental intervention $(\hat p_i^1)$. In the right plot, we have the corresponding contrasts $(p_i^1 - p_i^0)$. In both plots, the y-axis has been rank ordered by the magnitudes of the estimates. Other than how we have switched from predictions $\hat y_i$ to probabilities $\hat p_i$, the overall results of these plots follow the same patterns as those in their [analogues from the last post](https://timely-flan-2986f4.netlify.app/blog/2023-02-06-causal-inference-with-potential-outcomes-bootcamp/#estimands-estimators-and-estimates), where we used the conventional OLS framework. Because the logistic regression ANOVA model `glm1` has no covariates, the probabilities and their contrasts are identical for all participants. As we'll see later on, this will change when we switch to the ANCOVA model.

Another fine point that's easy to lose track of is all those $(p_i^1 - p_i^0)$ contrasts in the right plot are NOT individual causal effects $(\tau_i)$. For binary data, individual causal effects can only take on values of $-1$, $0$, or $1$. Even when we use the so-called standardization or g-computation method, our logistic regression models don't really return individual treatment effects, even in the counterfactual sense. Rather, they return individual probability contrasts. But importantly, the average of those probability contrasts does return a valid estimate of the ATE. Wild, huh?

Wrapping up, in the case of an ANOVA-type logistic regression model of a randomized experiment,

* $\operatorname{logit}^{-1}(\beta_0 + \beta_1) - \operatorname{logit}^{-1}(\beta_1)$,
* $p^1 - p^0$, and
* $\mathbb E (p_i^1 - p_i^0)$

are all the same thing. They're all estimators of our estimand $\tau_\text{ATE}$, the average treatment effect.

## ATE for the ANCOVA

In our [last post](http://localhost:4321/blog/2023-02-06-causal-inference-with-potential-outcomes-bootcamp/), we focused on a case where the only covariate was continuous. In this blog post, we're analyzing a data set containing a mixture of continuous and discrete covariates. So for notation sake, let $\mathbf C_i$ stand a vector of *continuous* covariates and let $\mathbf D_i$ stand a vector of *discrete* covariates, both of which vary across the $i$ cases. We can use these to help estimate the ATE with the formula:

$$\tau_\text{ATE} = \mathbb E (y_i^1 - y_i^0 \mid \mathbf C_i, \mathbf D_i).$$

In words, this means the average treatment effect in the population is the same as the average of each person's individual treatment effect, computed conditional on their continuous covariates $\mathbf C_i$ and discrete covariates $\mathbf D_i$. This, again, is sometimes called *standardization* or *g-computation*. Within the context of a logistic regression model, we further observe

$$
\tau_\text{ATE} = \mathbb E (y_i^1 - y_i^0 \mid \mathbf C_i, \mathbf D_i) = {\color{blueviolet}{\mathbb E (p_i^1 - p_i^0 \mid \mathbf C_i, \mathbf D_i)}},
$$

where $p_i^1$ and $p_i^0$ are the counterfactual probabilities for each of the $i$ cases, estimated in light of their covariate values. 

Whether we have continuous covariates, discrete covariates, or a combination of both, the standardization method works the same. However, this is no longer the case when using the difference in population means approach, the covariate-adjusted version of $\mathbb E (y_i^1) - \mathbb E (y_i^0)$. One complication is we might not be able to mean-center the discrete covariates in our $\mathbf D$ vector. Sometimes people will mean center dummy variables, which can lead to awkward interpretive issues[^9]. But even this approach will not generalize well to multi-categorical nominal variables, like ethnicity. Another solution is to set discrete covariates at their modes [see @muller2014estimating], which we'll denote $\mathbf D^m$. This gives us a new estimand:

$$\tau_\text{TEMM} = \operatorname{\mathbb{E}} \left (y_i^1 \mid \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (y_i^0 \mid \mathbf{\bar C}, \mathbf D^m \right),$$

where *TEMM* is an acronym for *treatment effect at the mean and/or mode*. Beware the TEMM acronym is not widely used in the literature; I'm just using it here to help clarify a point. More importantly, once you move beyond the ATE to specify particular values for $\mathbf C$ and/or $\mathbf D$, you're really just computing one form or another of the *conditional average treatment effect* (CATE; $\tau_\text{CATE}$), which we might clarify with the formula

$$\tau_\text{CATE} = \operatorname{\mathbb{E}}(y_i^1 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}}(y_i^0 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d),$$

where $\mathbf C = \mathbf c$ is meant to convey you have chosen particular values $\mathbf c$ for the variables in the $\mathbf C$ vector, and $\mathbf D = \mathbf d$ is meant to convey you have chosen particular values $\mathbf d$ for the variables in the $\mathbf D$ vector. In addition to means or modes, these values could be any which are of particular interest to researchers and their audiences.

Within the context of a logistic regression model, we further observe

$$
\begin{align*}
\tau_\text{TEMM} & = \operatorname{\mathbb{E}} \left (y_i^1 \mid \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (y_i^0 \mid \mathbf{\bar C}, \mathbf D^m \right) \\
& = {\color{blueviolet}{\left (p^1 \mid \mathbf{\bar C}, \mathbf D^m \right) - \left (p^0 \mid \mathbf{\bar C}, \mathbf D^m \right)}}
\end{align*}
$$

where $p_i^1$ and $p_i^0$ are the counterfactual probabilities for each of the $i$ cases, estimated in light of their covariate values. In a similar way

$$
\begin{align*}
\tau_\text{CATE} & = \operatorname{\mathbb{E}} (y_i^1 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}}(y_i^0 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d) \\
& = {\color{blueviolet}{(p^1 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d) - (p^0 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d)}}.
\end{align*}
$$

Importantly, we have some inequalities to consider:

$$
\begin{align*}
\mathbb E (y_i^1 - y_i^0 \mid \mathbf C_i, \mathbf D_i) & \neq \operatorname{\mathbb{E}} \left (y_i^1 \mid \mathbf{\bar C}, \mathbf D^m \right) - \operatorname{\mathbb{E}} \left (y_i^0 \mid \mathbf{\bar C}, \mathbf D^m \right) \\
& \neq \operatorname{\mathbb{E}} (y_i^1 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d) - \operatorname{\mathbb{E}} (y_i^0 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d)
\end{align*}
$$

and thus

$$
\begin{align*}
\mathbb E (p_i^1 - p_i^0 \mid \mathbf C_i, \mathbf D_i) & \neq \left (p^1 \mid \mathbf{\bar C}, \mathbf D^m \right) - \left (p^0 \mid \mathbf{\bar C}, \mathbf D^m \right) \\
& \neq (p^1 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d) - (p^0 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d),
\end{align*}
$$

which means

$$
\begin{align*}
\tau_\text{ATE} & \neq \tau_\text{TEMM} \\
& \neq \tau_\text{CATE}.
\end{align*}
$$

This holds for logistic regression models regardless of whether you have discrete covariates. But enough with theory. Let's move into application.

### $\beta_1$ in the logistic regression ANCOVA.

Earlier we learned the coefficient for the experimental group, $\beta_1$, does not have a direct relation with the ATE for the logistic regression ANOVA model. In a similar way, the $\beta_1$ coefficient does not have a direct relation with the ATE for the logistic regression ANCOVA model, either. If you want the ATE, you'll have to use the methods from the sections to come. In the meantime, let's compare the $\beta_1$ estimates for the ANOVA and ANCOVA models:

```{r}
bind_rows(tidy(glm1), tidy(glm2)) %>% 
  filter(term == "tx") %>% 
  mutate(fit = c("glm1", "glm2"),
         model_type = c("ANOVA", "ANCOVA")) %>%
  rename(`beta[1]` = estimate) %>% 
  select(fit, model_type, `beta[1]`, std.error)
```

Unlike what typically occurs with OLS-based models, the standard error for $\beta_1$ *increased* when we added the baseline covariates to the model. It turns out this will generally happen with logistic regression models, even when using high-quality covariates [@robinson1991someSurprising; see also @ford2002role]. This does not, however, mean we should not use baseline covariates in our logistic regression models. Rather, it means that we need to focus on how to compute the ATE, rather than fixate on the model coefficients [cf. @daniel2021makingApples]. This can be very unsettling for those with strong roots in the OLS framework--it was for me. All I can say is: Your OLS sensibilities will not help you, here. The sooner you shed them, the better.

### Compute $\left (p^1 \mid \mathbf{\bar C}, \mathbf D^m \right) - \left (p^0 \mid \mathbf{\bar C}, \mathbf D^m \right)$ from `glm2`.

With our ANCOVA-type `glm2` model, we can compute $\left (p^1 \mid \mathbf{\bar C}, \mathbf D^m \right)$ and $\left (p^0 \mid \mathbf{\bar C}, \mathbf D^m \right)$ with the base **R** `predict()` function. As a first step, we'll define our prediction grid with the sample mean for our continuous covariate `agez`, the sample modes for our four discrete covariates, and then expand the grid to include both values of the `experimental` dummy. This presents a small difficulty, however, because base **R** does not have a function for modes. Here we'll make one ourselves.

```{r}
get_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

This `get_mode()` function is used internally by the **marginaleffects** package (see [here](https://github.com/vincentarelbundock/marginaleffects/blob/9a06aa03c017947df978caa4d82fa6e650e2de8f/R/mean_or_mode.R#L4)), and has its origins in [this](https://stackoverflow.com/a/8189441/342331) stackoverflow discussion. Here's how we can use `get_mode()` to help us make the `nd` data grid.

```{r}
nd <- wilson2017 %>% 
  summarise(agez      = 0,  # recall agez is a z-score, with a mean of 0 by definition
            gender    = get_mode(gender),
            msm       = get_mode(msm),
            ethnicgrp = get_mode(ethnicgrp),
            partners  = get_mode(partners)) %>% 
  expand_grid(tx = 0:1)

# what is this?
print(nd)
```

Thus we will be computing our estimate for $\tau_\text{TEMM}$ based on a White 23-year-old woman who had one partner over the past year. By definition, such a person would not be a man who has sex with men (`msm == 1`). Also, we know this person is 23 years old because `agez == 0` at that value. Here's the proof.

```{r}
wilson2017 %>% 
  summarise(mean_age = mean(age))
```

Now we pump these values into `predict()`.

```{r}
predict(glm2, 
        newdata = nd,
        se.fit = TRUE,
        type = "response") %>% 
  data.frame() %>% 
  bind_cols(nd)
```

To get the contrast with standard errors and so on, we switch to the `predictions()` function and set `hypothesis = "revpairwise"`.

```{r}
# conditional probabilities
predictions(glm2, newdata = nd, by = "tx")

# TEMM
predictions(glm2, newdata = nd, by = "tx", hypothesis = "revpairwise")
```

Thus we expect our hypothetical person with demographics at the mean and/or modes for the covariates will be about 22% more likely to get tested if given the intervention, compared to if she had not.

### Compute $(p^1 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d) - (p^0 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d)$ from `glm2`.

Since the $\tau_\text{TEMM}$ is just a special case of a $\tau_\text{CATE}$, we might practice computing our estimate for $\tau_\text{CATE}$ with a different set of covariate values. Men who have sex with men (MSM) were one of the vulnerable subgroups of interest in @wilson2017internet, so we might take a look to see which combination of covariate values was most common for MSM in our subset of the data.

```{r}
wilson2017 %>% 
  filter(msm == "msm") %>% 
  count(age, agez, ethnicgrp, partners) %>% 
  arrange(desc(n))
```

It appears we're now interested in computing $\tau_\text{CATE}$ for a White 26-year-old MSM who had 10 or more partners over the past year. Let's redefine our `nd` predictor grid accordingly.
 
```{r}
nd <- wilson2017 %>% 
  filter(msm == "msm") %>% 
  count(age, agez, gender, msm, ethnicgrp, partners) %>% 
  arrange(desc(n)) %>% 
  slice(1) %>% 
  select(-n) %>% 
  expand_grid(tx = 0:1)

# what now?
print(nd)
```

Now use `predictions()` to compute the counterfactual probabilities and the $\tau_\text{CATE}$.

```{r}
# conditional probabilities
predictions(glm2, newdata = nd, by = "tx")

# CATE
predictions(glm2, newdata = nd, by = "tx", hypothesis = "revpairwise")
```

Turns out this $\tau_\text{CATE}$ is a little larger than our estimate for $\tau_\text{TEMM}$, from above. With this framework, you can compute $\tau_\text{CATE}$  estimates for any number of theoretically-meaningful covariate sets.

### Compute $\mathbb E (p_i^1 - p_i^0 \mid \mathbf C_i, \mathbf D_i)$ from `glm2`.

Before we compute our counterfactual $\mathbb{E}(p_i^1 - p_i^0 \mid \mathbf C_i, \mathbf D_i)$ estimates from our ANCOVA-type logistic regression model `glm2`, we'll first need to redefine our `nd` predictor data. This time, we'll retain the full set of covariate values for each participant.

```{r}
nd <- wilson2017 %>% 
  select(id, age, agez, gender, msm, ethnicgrp, partners) %>% 
  expand_grid(tx = 0:1)

# what?
glimpse(nd)
```

Instead of first practicing computing the probabilities with base **R** `predict()`, let's just jump directly to the `precitions()` and `comparisons()` functions from the **marginaleffects** package.

```{r}
# here are the probabilities
predictions(glm2, newdata = nd) %>% 
  head(n = 10)

# here are the contrasts based on those probabilities
comparisons(glm2, newdata = nd, variables = "tx") %>% 
  head(n = 10)
```

Even among the first 10 rows, we can see there's a lot of diversity among the estimates for the individual treatment effects. Before we compute the ATE, it might be worth the effort to look more closely at the person-level estimates in a coefficient plot. As with the ANOVA model, we'll only visualize an $n = 50$ subset of the 400 cases.

```{r, fig.width = 8, fig.height = 5.5}
# counterfactual probabilities
p3 <- predictions(glm2, newdata = nd) %>% 
  data.frame() %>% 
  filter(id %in% id_subset) %>% 
  mutate(y = ifelse(tx == 0, "hat(italic(p))^0", "hat(italic(p))^1")) %>% 
  
  ggplot(aes(x = estimate, y = reorder(id, estimate), color = y)) +
  geom_interval(aes(xmin = conf.low, xmax = conf.high),
                position = position_dodge(width = -0.2),
                size = 1/5) +
  geom_point(aes(shape = y),
             size = 2) +
  scale_color_viridis_d(NULL, option = "A", begin = .3, end = .6,
                        labels = scales::parse_format()) +
  scale_shape_manual(NULL, values = c(20, 18),
                     labels = scales::parse_format()) +
  scale_x_continuous(limits = 0:1) +
  scale_y_discrete(breaks = NULL) +
  labs(subtitle = "Counterfactual probabilities",
       x = expression(italic(p[i])),
       y = "id (ranked)") +
  theme(legend.background = element_blank(),
        legend.position = c(.9, .85))

# treatment effects
p4 <- comparisons(glm2, newdata = nd, variables = "tx") %>% 
  data.frame() %>% 
  filter(id %in% id_subset) %>% 
  
  ggplot(aes(x = estimate, y = reorder(id, estimate))) +
  geom_vline(xintercept = 0, color = "white") +
  geom_interval(aes(xmin = conf.low, xmax = conf.high),
                size = 1/5) +
  geom_point() +
  scale_x_continuous(limits = c(-0.5, 0.5)) +
  scale_y_discrete(breaks = NULL) +
  labs(subtitle = "Contrasts",
       x = expression(hat(italic(p))[italic(i)]^1-hat(italic(p))[italic(i)]^0),
       y = NULL) +
  theme(legend.background = element_blank(),
        legend.position = c(.9, .85))

# combine
p3 + p4 + plot_annotation(title = "Person-level estimates based on the logistic regression ANCOVA model (glm2)")
```

Now we have added covariates to the model, the counterfactual probabilities vary across participants, which was the same pattern for the OLS-based ANCOVA from the [last post](https://timely-flan-2986f4.netlify.app/blog/2023-02-06-causal-inference-with-potential-outcomes-bootcamp/#counterfactual-interventions-with-covariates), too. But unlike with the ANOVA model and unlike with the ANCOVA results from the last post, the contrasts $(p_i^1 - p_i^0)$ now vary across participants. Once you leave the simple OLS paradigm, this issue will come up again and again when you fit ANCOVA models. Covariate values change the magnitudes of the probability estimates and their contrasts.

Investigating further, here's the full distribution of the contrast values for all $n = 400$ cases. To reduce visual complexity, we'll drop the 95% confidence interval lines.

```{r, fig.width = 4.5, fig.height = 4}
comparisons(glm2, newdata = nd, variables = "tx", by = "id") %>% 
  data.frame() %>% 
  
  ggplot(aes(x = estimate)) +
  geom_vline(xintercept = 0, color = "white") +
  geom_dots(layout = "swarm", color = "gray30", fill = "gray30") +
  stat_pointinterval(aes(y = -0.017), 
                     point_interval = mean_qi, .width = .5, 
                     point_size = 2.5, color = "red") +
  scale_x_continuous(limits = c(-0.5, 0.5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "The individual contrast distribution",
       subtitle = "Each gray dot is a point estmiate for a single participant's\ncontrast. The horizontal red line marks off the interquartile\nrange, and the red dot marks the ATE.",
       x = expression(hat(italic(p))[italic(i)]^1-hat(italic(p))[italic(i)]^0)) +
  coord_cartesian(ylim = c(0, 0.7))
```

The red dot below the distribution marks off the average of the participant-level probability contrast estimates, which is the same as the point estimate for the ATE. Speaking of which, here's $\tau_\text{ATE}$ for this model, and for the simpler ANOVA-type `glm1`.

```{r}
bind_rows(
  avg_comparisons(glm1, newdata = nd, variables = "tx"),
  avg_comparisons(glm2, newdata = nd, variables = "tx")
) %>% 
  data.frame() %>% 
  mutate(fit = c("glm1", "glm2"),
         model_type = c("ANOVA", "ANCOVA")) %>%
  rename(`tau[ATE]` = estimate) %>% 
  select(fit, model_type, `tau[ATE]`, std.error)
```

Whereas the standard error for the $\beta_1$ coefficient *increased* when we added the baseline covariates to the model, the standard error for our primary estimand $\tau_\text{ATE}$ *decreased*. This isn't a fluke of our $n = 400$ subset. The same general pattern holds for the full data set. Not only is $\beta_1$ not the same as the ATE for a logistic regression model, adding covariates can have the reverse effect on their respective standard errors. This phenomena is related to the so-called noncollapsibility issue, which is well known among statisticians who work with medical trials. For an entry point into that literature, see @daniel2021makingApples or @morris2022planning. But anyway, yes, baseline covariates can help increase the precision with which you estimate the ATE from a logistic regression model. Don't worry about what happens with $\beta_1$. Focus on the ATE.

### Grappling with $(p_i^1 - p_i^0 \mid \mathbf C_i, \mathbf D_i)$ distributions.

Given how the logistic-regression-based participant-level probability contrasts now come in distributions when estimated from ANCOVA models, some researchers have wondered whether it's a good idea to use a single summary value like the ATE. Biostatistician Frank Harrell, for example, recommended displaying the entire contrast distribution in his [-@harrell2021avoiding] blog post, [*Avoiding one-number summaries of treatment effects for RCTs with binary outcomes*](https://www.fharrell.com/post/rdist/). Albuquerque and Arel-Bundock covered Harrell's primary material from a **marginaleffects** perspective in their [-@albuquerque2023logisticRegression] vignette, [*Logistic regression*](https://vincentarelbundock.github.io/marginaleffects/articles/logit.html) [see also @kent2007limitations]. At the moment, I'm inclined to still rely on the ATE, but perhaps also show a plot of the contrast distribution as a supplement. 

Another approach might be to focus on one or a handful of CATE's. This, however, I would only recommend with great caution. To help clarify why, let's compute the CATE for every valid combination of our baseline covariate values. First, we'll update our `nd` data grid.

```{r}
nd <- crossing(
  agez      = distinct(wilson2017, agez) %>% pull(),
  gender    = distinct(wilson2017, gender) %>% pull(),
  msm       = distinct(wilson2017, msm) %>% pull(),
  ethnicgrp = distinct(wilson2017, ethnicgrp) %>% pull(),
  partners  = distinct(wilson2017, partners) %>% pull()) %>% 
  # remove the impossible cases of Females who are also MSM
  filter((gender == "Female" & msm == "other") | gender == "Male") %>% 
  # throw in an id index
  mutate(id = 1:n()) %>% 
  expand_grid(tx = 0:1)

# what?
glimpse(nd)
```

Now we'll summarize the results with a plot of the point estimates (left), and a plot their standard errors (right).

```{r CATE_distributions, fig.width = 8, fig.height = 4.25}
# compute/save the point estimate and standard error for the ATE
ate_est <- avg_comparisons(glm2, variables = "tx") %>% pull(estimate)
ate_se <- avg_comparisons(glm2, variables = "tx") %>% pull(std.error)

# point estimates
p5 <- comparisons(glm2, newdata = nd, variables = "tx", by = "id")  %>% 
  data.frame() %>% 
  
  ggplot(aes(x = estimate)) +
  geom_histogram(boundary = 0, binwidth = 0.01) +
  geom_vline(xintercept = ate_est, color = "red") +
  scale_y_continuous(limits = c(0, 450)) +
  labs(subtitle = "The red line marks the ATE point estimate.",
       x = expression(widehat(CATE)))

# standard errors
p6 <- comparisons(glm2, newdata = nd, variables = "tx", by = "id")  %>% 
  data.frame() %>% 
  
  ggplot(aes(x = std.error)) +
  geom_histogram(boundary = 0, binwidth = 0.005) +
  geom_vline(xintercept = ate_se, color = "red") +
  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 450)) +
  labs(subtitle = "The red line marks the ATE standard error.",
       x = expression(CATE[italic(SE)]))

# combine
p5 + p6 + 
  plot_annotation(title = "CATE distributions based on the logistic regression ANCOVA model (glm2)",
                  subtitle = "Based on 2,250 possible combinations of covariate values.")
```

The ATE is still near the middle of the distribution of point estimates for the CATE's. However, the standard error for the ATE is well lower than the bulk of the standard errors for the various versions of the CATE. If you have designed and powered a study to compute the ATE, be very cautious about switching your focus to the CATE. You might not have the right data set to answer questions like that.

In case you were wondering, all those counterfactual cases with near-zero point estimates and near-zero standard errors had `Other` as the value for `ethnicgrp`. If you look back up to the model summary for `glm2`, you'll notice the coefficient for that category has a very low point estimate and an extremely large standard error. This is what can happen when you include a categorical variables with only a handful of cases for one of the categories in a frequentist logistic regression model. Happily, this will be a much smaller problem when we adopt a Bayesian framework in the next post.

```{r, eval = F, echo = F}
# which cases are the problem?
id_low_se <- comparisons(glm2, newdata = nd, variables = "tx", by = "id")  %>% 
  data.frame() %>% 
  filter(std.error < 0.02) %>% 
  pull(id)

nd %>% 
  filter(id %in% id_low_se)

# check with point estimates
all.equal(
  comparisons(glm2, newdata = nd, variables = "tx", by = "id")  %>% 
    data.frame() %>% 
    filter(estimate < 0.03) %>% 
    pull(id),  
  id_low_se)  # yep, same

# check the model summary again
summary(glm2)

# how many cases have ethnicgrp == "Other"
wilson2017 %>% 
  count(ethnicgrp)

# answer: it's 4
```

## Words are hard

Before we wrap up, y'all should beware the language of *ATE* is not uniformly used among researchers who use logistic regression for causal inference. For example, Gelman and colleagues used the language of "the difference in probabilities" on page 225 of their [-@gelmanRegressionOtherStories2020] textbook. @mccabe2022interpreting used the language of *discrete differences* "to define a marginal effect as the difference between two points on a regression function" (p. 247), which is what we're doing in the special case of a randomized experiment. @agresti2018simple used the term *discrete change* for discrete variables and *average marginal effect* for continuous ones. @mood2010logistic, differentiated discrete and continuous models by calling the ATE from a logistic regression model $\Delta{P}$, and using the term *average marginal effect* when computing the ATE from a conventional Gaussian model.

## Acknowledgments

I became aware of @wilson2017internet through the follow-up paper by @morris2022planning. Morris and colleagues compared several ways to analyze these data, one of which was the standardization approach for logistic regression, such as we have done here. However, Morris and colleagues used a STATA-based workflow for their paper, and it was [A. Jordan Nafa](https://www.ajordannafa.com/)'s kind efforts (see [here](https://github.com/ajnafa/morris-et-al-2022-replication)) which helped me understand how to use these methods in **R**.

## Recap

In this post, some of the main points we covered were:

* With logistic regression, the $\beta_1$ coefficient has no direct relationship with the ATE, regardless of whether you have included covariates.
* For the logistic regression ANOVA model,
  - $\tau_\text{ATE} = \mathbb E (p_i^1 - p_i^0)$, and
  - $\tau_\text{ATE} = p^1 - p^0$.
* For the logistic regression ANCOVA model,
  - $\tau_\text{ATE} = \mathbb E (p_i^1 - p_i^0 \mid \mathbf C_i, \mathbf D_i)$, but
  - $\tau_\text{CATE} = (p^1 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d) - (p^0 \mid \mathbf C = \mathbf c, \mathbf D = \mathbf d)$.
* For a logistic regression ANCOVA model, there can be many different values for the conditional average treatment effect, $\tau_\text{CATE}$, depending which values one uses for the covariates. 
* With logistic regression models, baseline covariates tend to
  - *in*crease the standard errors for the $\beta_1$ coefficient, and
  - *de*crease the standard errors for the average treatment effect, $\tau_\text{ATE}$.

In the [next post](https://timely-flan-2986f4.netlify.app/blog/2023-02-15-causal-inference-with-bayesian-models/), we'll explore how our causal inference methods work within an applied Bayesian statistics framework. We'll practice with both simple Gaussian models, and logistic regression models, too. Until then, happy modeling, friends!

## Session information

```{r sessionInfo}
sessionInfo()
```

## References

[^1]: Yes, you geeks, I know we could also use the Bernoulli distribution. But the binomial is much more popular and if we're going to rely on the nice base **R** `glm()` function, we'll be setting `family = binomial`. There is no option for `family = bernoulli`.

[^2]: I suppose you could even argue it's a censored count. But since we'll be using it as a predictor, I'm not sure that argument would be of much help.

[^3]: As it turns out, statisticians and quanty researchers are not in total agreement on whether or how one must condition on covariates when those covariates were used to balance during the randomization process. For a lively twitter discussion on this very data set, see the replies to [this twitter poll](https://twitter.com/SolomonKurz/status/1623349977786228736).

[^4]: At this point, one might ask: *Which covariates should I include in my ANCOVA?* At the moment, I'm in large agreement with @raab2000HowToSelect, who recommend you condition on covariates that are theorized or have been empirically shown to be strongly predictive of the outcome, and/or where used to balance during the randomization process. They further added researchers would do well by planning their covariate set before data collection, and publicly reporting their plan in some kind of pre-registration report, which would help reduce $p$-hacking and other such nonsense.

[^5]: At the moment (03-26-2023), it appears the **flextable** package (0.9.0) isn't playing nicely with **blogdown** (1.16). Sadly, I don't have the technical abilities to understand why, but I get insurmountable error warnings when trying to render. For now I'm leaving the **flextable** code in, in the hopes the conflict will find resolution, soon.

[^6]: There are numerous effect sizes one could compute from a logistic regression model. For a more exhaustive list, as applied within our causal inference framework, see Section 3.3 in @brumback2022Fundamentals.

[^7]: In some parts of the literature, probabilities are called "risks" and differences in probabilities are called "risk differences" [e.g., @morris2022planning]. We will not be using the jargon of "risk" in this blog series.

[^8]: Note that the standard deviation, here, isn't quite the same thing as a standard error. We'd need to do something more akin to bootstrapping, for that. However, this kind of a workflow does have some things in common with the Monte-Carlo-based Bayesian methods we'll be practicing later in this series.

[^9]: For example, say you have a dummy variable called `male`, which is a zero for women and a one for men. One way to interpret a model using the centered version of `male` is it returns the contrast weighted by the proportion of women/men in the sample or population. Another interpretation is this returns the contrast for someone who is in the middle of the female-male spectrum--which is what? Intersex? Non-binary? Transgender? It might be possible to interpret such a computation with skill and care, but such an approach might also leave one's audience confused or offended.

```{r, fig.width = 4, fig.height = 4, echo = F, eval = F}
comparisons(glm2, variables = "tx") %>% 
  data.frame() %>% 
  
  ggplot(aes(x = estimate)) +
  tidybayes::geom_dots(layout = "swarm", 
                       fill = "magenta4", color = "magenta4") +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "The individual treatment effect distribution",
       x = expression(hat(tau)[italic(i)]~("i.e., "*hat(italic(p))[italic(i)]^1-hat(italic(p))[italic(i)]^0))) +
  theme_gray(base_size = 11.7, base_family = "serif") +
  theme(panel.background = element_rect(fill = "#ffe5ea"),
        panel.grid = element_blank())

ggsave("tau-i-featured.jpg", width = 3.5, height = 3.5, units = "in")
```

